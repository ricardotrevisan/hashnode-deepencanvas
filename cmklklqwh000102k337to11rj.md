---
title: "But before you startâ€¦"
datePublished: Mon Jan 19 2026 19:40:53 GMT+0000 (Coordinated Universal Time)
cuid: cmklklqwh000102k337to11rj
slug: but-before-you-start
cover: https://raw.githubusercontent.com/ricardotrevisan/ricardotrevisan.github.io/master/images/image_1_20260119.png
tags: ai, analytics, ml, mlops, aiops, aiinfrastructure, techleadership, enterpriseai

---

> **â€œThe bottleneck is not the models themselves. Whatâ€™s holding enterprises back is the surrounding infrastructure.â€**

A recent publication by MIT Technology Review perfectly summarizes the current state of corporate AI.

Weâ€™re not held back by a lack of better models. Weâ€™re held back by **decisions made too early** â€” and perpetuated by a mistaken reading of the problem.

The frustration that many companies are starting to feel with AI does not stem from technical failure. On the contrary: LLMs work, RAGs respond, demos impress. The impasse arises when these initiatives try to leave the *showcase* and run into a reality built on inaccessible data, rigid integrations, brittle pipelines, and deployment paths that donâ€™t tolerate change, error, or scale.

The bottleneck isnâ€™t the model.  
Itâ€™s the infrastructure â€” technical *and* organizational â€” that should support it.

---

We need to widen the lens to the **moment before deciding to invest in AI**. The problem begins before the technology: in the executive read that makes explicit â€” or not â€” the motivation behind adoption initiatives, regardless of the tool chosen.

### Meta-layer 1 â€” Reframing the Conversation

Before any technical analysis, itâ€™s necessary to redefine how AI is discussed within the organization.

AI is not a permanent experiment.  
â€œPilotâ€ is not a success.  
Models are replaceable; architecture, not.  
Value must be expressed in **process economics**, not in isolated metrics.

Without this reframing, any framework becomes theater.

---

### Meta-layer 2 â€” Strategic Framing

Every initiative must arise from a **real business pressure**: margin, scale, risk, talent scarcity. Technology only comes after â€” as a response, not a trigger.

Yet many organizations start their journey by choosing models, comparing technical benchmarks, and debating sophisticated architectures even before answering basic questions: *which process is under pressure?*, *which result needs to change?*, *which metric defines success or failure?*

When this start is wrong, everything that follows inherits the distortion. The absence of minimum cycles to measure results â€” short, objective, and tied to business metrics â€” makes the initiative age poorly. What was promised becomes cost. What was an experiment becomes debt.

And the more time and money invested, the greater the pressure to â€œmake it work.â€

**The most expensive mistake in AI adoption is not failing fast.** Itâ€™s **starting wrong and continuing to move forward as if the problem were somewhere else**.

---

## New names for familiar elements

Itâ€™s worth breaking the crystallized sense that weâ€™re always talking only about LLMs and natural-language models. The bucket called â€œAIâ€ is much broader and gathers diverse, already mature capabilities applicable to internal processes.

Computer vision models identify people, objects, and material states in real time. Audio models enable sound classification, speech recognition and synthesis. Add to that the full suite of classic Machine Learning capabilities â€” regression, classification, *clustering*, *forecasting*, *anomaly detection*, *recommendation* â€” which not only remain relevant, but have evolved significantly.

All of this is AI.  
And today itâ€™s broadly accessible â€” largely via *open source*.

Thereâ€™s a rarely stated point: **Machine Learning is, for the most part, the direct continuation of what we used to call Analytics**. The boundary was never technical. It was organizational.

Forecasting, segmentation, anomaly detection, propensity models, recommendation, optimization â€” all of these have belonged to the analytic repertoire for years. What changed isnâ€™t the nature of the problem, but the sophistication of methods and the scale of data available.

And hereâ€™s the uncomfortable truth:  
**those who failed to extract value from Analytics will continue to fail with AI â€” for the same reasons.**

Not due to technical incapacity, but because:

* the processes were never clearly defined,
    
* the data were never treated as business assets,
    
* the results were never linked to real decisions,
    
* and accountability was always pushed to the tool.
    

AI doesnâ€™t fix these failures. It **exposes them faster and at a higher cost**.

This helps explain the resistance of many seasoned professionals. For those who lived through BI, Data Warehousing, and Analytics cycles that promised more than they delivered, AI sounds like â€œmore of the sameâ€ â€” just with a different name and more hype. And, without structural reframing, that reading isnâ€™t irrational.

The difference is that, this time, the technology works better than ever. The risk isnâ€™t in the modelsâ€™ capacity, but in repeating the same mistakes â€” now at a larger scale.

---

## The side effect: blaming the model

Thatâ€™s where the most visible â€” and most deceptive â€” symptom appears.

> â€œThis model isnâ€™t good enough. We need to replace it.â€

Constantly swapping models is rarely a strategic decision. More often, itâ€™s **a time-gain strategy**. A postponement of the hard talk about architecture, process, governance, and economics.

Thus the circle of fools begins. The model changes, but:

* the process remains poorly defined,
    
* the data stay fragmented,
    
* the integration stays fragile,
    
* and deployment remains artisanal.
    

Costs rise. Frustration grows. And the diagnosis stays wrong.

---

## What market critique gets right â€” and where it must move forward

Analyses today rightly point out that the limitation isnâ€™t in the models, but in the surrounding infrastructure. This shifts the focus from the technology itself to the system that should sustain it.

But thereâ€™s one more step: recognize that **infrastructure isnâ€™t only technical**. Itâ€™s also conceptual, strategic, and organizational. Without this, the risk is simply swapping model hype for platform hype â€” and repeating the same mistake at another level.

---

## The path: a structured adoption plan

AI stops being a â€œspecial projectâ€ when itâ€™s treated as an **operational capability**. And that requires a clear adoption plan to guide decisions before they become irreversible.

This plan can be understood in two complementary levels: **meta-layers**, which shape behavior and language, and **gates**, which discipline decisions.

---

### The gates: from reality to technical decision

With the meta-layers in place, adoption can advance in a disciplined way.

**Gate 1 â€” Process Anchoring**  
Clearly define the process under pressure, its cost, volume, error tolerance, and owner. Without this, there is no AI case.

**Gate 2 â€” Contextual Market Benchmarking**  
See how comparable companies faced similar pressures â€” not to copy results, but to understand **decision patterns**, prerequisites, and limits. Benchmarks here illuminate paths; they donâ€™t define destinations.

**Gate 3 â€” Economic Viability**  
Test whether AI actually changes the economics of the process, considering simpler alternatives. If it isnâ€™t the best option, stopping is a sign of maturity.

**Gate 4 â€” Architecture and Operating Model**  
Ensure the solution can be operated, monitored, audited, and shut down. Temporary architecture is debt guaranteed.

**Gate 5 â€” Model Selection and Technical Benchmarks**  
Only here do performance benchmarks enter â€” as minimum acceptance criteria, cost-aware, and fault-aware. The best model isnâ€™t the most advanced, but the most suited to the system.

---

### Meta-layer final â€” Continuous reality check

After go-live, business KPIs take the lead. Technical benchmarks serve only to detect regression. And AI needs to be reducible or removable without trauma.

Infrastructure can be decommissioned.  
â€œStrategic initiatives,â€ typically, cannot.

---

## Conclusion: less hype, more system

The current frustration with AI isnâ€™t a sign of tech fatigue. Itâ€™s a sign of forced maturation. The market is starting to realize there are no technical shortcuts for a structural problem.

Starting right â€” with reframing, strategy, measurement cycles, and incremental evolution of complexity â€” doesnâ€™t guarantee automatic success. But it avoids the worst failure of all: spending time and money trying to prove a promise that only grows more expensive as it ages.

The quiet AI revolution isnâ€™t in the models. Itâ€™s in how organizations decide to adopt them.

ğŸ“š Sources:

* MIT Technology Review: https://www.technologyreview.com/2026/01/19/1131422/going-beyond-pilots-with-composable-and-sovereign-ai/
    

#AI #EnterpriseAI #AIOps #AIInfrastructure #Analytics #ML #MLOps #TechLeadership