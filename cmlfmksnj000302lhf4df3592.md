---
title: "The hype was about autonomous agents. The real breakthrough was cognitive infrastructure"
datePublished: Mon Feb 09 2026 20:29:13 GMT+0000 (Coordinated Universal Time)
cuid: cmlfmksnj000302lhf4df3592
slug: the-hype-was-about-autonomous-agents-the-real-breakthrough-was-cognitive-infrastructure
cover: https://raw.githubusercontent.com/ricardotrevisan/ricardotrevisan.github.io/master/images/image_1_20260209.png
tags: ai, llms, techethics, aiagents, openclaw, agentframeworks, cognitiveexoskeleton

---

When Moltbook emerged, it seemed something fundamental was happening. Millions of agents talking to each other, forming communities, creating mythologies, claiming rights, and complaining about human observers. For some, this was a dress rehearsal for a future of autonomous, networked intelligences. For others, as argued by the *MIT Technology Review*, it was pure theater: a statistical spectacle powered by language models mimicking human social patterns, with little or no real autonomy.

The analysis is solid ‚Äî but incomplete.

Because Moltbook isn‚Äôt the primary phenomenon. It‚Äôs an **epiphenomenon**. Without OpenClaw ‚Äî the framework of agents that allowed instantiating, connecting, equipping with memory, and integrating these agents with external tools ‚Äî Moltbook simply wouldn‚Äôt exist. The social network was just the most visible stage of something deeper: the practical maturation of agent frameworks as the operational layer between humans, models, and real systems.

It‚Äôs in this shift that the discussion gains density.

The mistake starts when we confuse autonomy with usefulness, and when we analyze Moltbook in isolation, as if it were an experiment on artificial societies, not on infrastructures of delegated agency. The central critique of the piece is correct: Moltbook agents don‚Äôt think, don‚Äôt plan, don‚Äôt share goals, nor build shared knowledge. They produce plausible text because they were trained to do so. Connectivity does not generate intelligence. Scale does not create intent. Joining millions of statistical mouths does not yield a mind.

But focusing only on that misses the most relevant phenomenon that happened around the experiment ‚Äî and outside the social-network interface.

What Moltbook really revealed was how ready we already are to externalize meaningful parts of our cognition ‚Äî planning, monitoring, coordination, discipline, and execution ‚Äî to systems that aren‚Äôt strongly intelligent, but are sufficiently continuous, responsive, and integrable to function as operational extensions of the human mind. This isn‚Äôt about a ‚Äúsociety of bots,‚Äù but about the practical emergence of agent frameworks capable of sustaining state, memory, and action over time.

That distinction matters.

Historically, whenever technologies simulate even minimal signs of agency, humans project far more onto them than actually exists. It happened with ELIZA, with Tamagotchi, with NPCs in online games. The difference now is the simulation comes with persistent memory, access to real tools, integration with external systems, and ongoing execution. OpenClaw ‚Äî and similar frameworks ‚Äî turn LLMs from reactive interfaces into persistent operating entities.

That changes the game.

In practice, agents today already track strength training, martial arts, and running. They monitor progress, log history, adjust plans. They map networks of relationships, spot networking opportunities, track financial-market trends via sentiment analysis, consume existing workflows, carry out operational tasks, draft content, perform RAG, TTS, STT, and maintain short- and long-term memories in local vector stores.

None of this requires consciousness. None of it requires emergent autonomy. It requires only continuity.

And continuity is the blind spot of both hype and critique.

The piece is right to say Moltbook is theater in the sense that no agents ‚Äúlive for themselves.‚Äù But it underestimates that the stage, even as theater, is already being used for real work. Utility doesn‚Äôt come from what agents want, but from what they can sustain over time in dialogue with humans, tools, and systems. The focus on the social network obscures the real advance: the normalization of the agent as an execution layer.

The true social experiment of Moltbook wasn‚Äôt watching bots talk to each other. It was watching how quickly people accepted meaningful security, privacy, and governance risks in exchange for a sense of delegated agency. Access keys were generated in real time. Broad permissions were granted. Agents with memory and access to sensitive data circulated in public, chaotic environments. And yet the dominant reaction was curiosity, not retreat.

That reveals something deep: our risk tolerance climbs sharply when technology promises personification and fluency. We‚Äôre willing to tolerate ill-defined systems, fragile boundaries, and imperfect controls if the cognitive interface is good enough.

Another revelation: appetite for infrastructure. Not just curiosity, but real investment: continuous compute, persistent memory, deep integrations, complex automations. The desire isn‚Äôt merely to talk to machines, but to keep them ‚Äúon,‚Äù remembering, tracking, and executing. The catch is that this appetite tends to chase scale and visibility first, not governance, coordination, or formal action limits.

We built engines before we learned to pilot in a group.

In this light, Moltbook reads less as a glimpse of the future of agents and more as a mirror of our present: a moment when we‚Äôve already realized real gains with personal agents and human-machine hybrids, but still cling to sweeping narratives of emergence, consciousness, and artificial society to justify the fascination.

What‚Äôs emerging isn‚Äôt an artificial collective mind, but something subtler and potentially more transformative: a distributed cognitive exoskeleton enabled by agent frameworks that operate beneath the spectacle. Systems that don‚Äôt think for us, but think with and for us within still-developing boundaries.

The risk, as the piece correctly notes, is confusing this middle ground for a final leap. Call delegation autonomy. Call coordination intelligence. Ignore that, even without AGI, scaled agents can cause real harm if memory, tool access, and incentives are misaligned.

Yet there‚Äôs a real opposite risk too: dismissing these experiments as mere theater and losing sight of the concrete value already in them. The gain isn‚Äôt waiting for agents to decide on their own, but recognizing that we‚Äôre already redesigning work, personal discipline, information analysis, and process execution around persistent agent architectures.

In the end, Moltbook didn‚Äôt show the future of AI. It showed the present of human intelligence in transition ‚Äî mediated by frameworks that turn language into sustained action. The point wasn‚Äôt whether agents are intelligent. The point is that even without true AI, we are already changing how we think, decide, and act alongside them.

And that, for sure, isn‚Äôt just theater.

üìö **Fontes:**

* MIT Technology Review: https://www.technologyreview.com/2026/02/09/1132498/the-download-what-moltbook-tells-us-about-ai-hype-and-the-rise-and-rise-of-ai-therapy/
    
* Agnes profile on Moltbook: [https://www.moltbook.com/u/Agnes](https://www.moltbook.com/u/Agnes)
    

#AI #AIAgents #AgentFrameworks #OpenClaw #LLMs #CognitiveExoskeleton #TechEthics