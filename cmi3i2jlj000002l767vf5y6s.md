---
title: "Remember that topic?"
datePublished: Mon Nov 17 2025 18:50:42 GMT+0000 (Coordinated Universal Time)
cuid: cmi3i2jlj000002l767vf5y6s
slug: remember-that-topic-1
cover: https://raw.githubusercontent.com/ricardotrevisan/ricardotrevisan.github.io/master/images/image_20251104.png
tags: ai, machinelearning, cognitivecomputing

---

Remember that topic?

When two people say that, they don‚Äôt relive every frame of the past.
They revisit relevant traces of their joint history ‚Äî evoking contexts, choosing what matters to evolve the current conversation.

Now imagine two LLM models doing the same.

In the world of language models, ‚Äúremembering‚Äù is about cached representations (K/V) ‚Äî the semantic memory space where the model stores representations of what it has processed.
The Query (Q) is the trigger: ‚ÄúHey, remember this; it might matter now.‚Äù

That memory sharing fuels attention, reduces latency and cost, and opens space for something new: models talking directly, without intermediaries.

Research from Tsinghua University shows that, to evolve logical and cumulative reasoning, it‚Äôs not enough to have a Chain of Thought or Tree of Thought.
What is needed is a memory space ‚Äî an organized place to store intermediate results.

It would be like dialoguing without preserving previous answers in a structured and familiar space ‚Äî we would quickly lose the thread of the narrative.
The Cumulative Reasoning (CR) framework from Tsinghua introduces this ‚Äúmemory space‚Äù, dramatically improving inference capacity.

In essence, each LLM queries what the other has already ‚Äúlived‚Äù, sharing context without reprocessing everything. Less redundancy, more fluency, more collective intelligence.

The challenge ‚Äî and the beauty ‚Äî lies in deciding how much of the history to carry and at what level of detail. We do this naturally in our conversations: we surface only what adds meaning to the next step.

It‚Äôs interesting how innovation has a deep relation to how we understand ourselves.

The success of these frameworks depends on this subtle balance between what needs to be remembered ‚Äî and what deserves to be forgotten. Like life itself, right?

üìö **Sources:**
‚Ä¢ Marktechpost - C2C: https://www.marktechpost.com/2025/11/04/cache-to-cachec2c-direct-semantic-communication-between-large-language-models-via-kv-cache-fusion/


Hashtags
#AI #CognitiveComputing #MachineLearning