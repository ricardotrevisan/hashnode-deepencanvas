---
title: "From "how much" to "whether""
datePublished: Tue Dec 16 2025 23:09:09 GMT+0000 (Coordinated Universal Time)
cuid: cmj972m9z000702l7h6bt20x0
slug: from-how-much-to-whether
cover: https://raw.githubusercontent.com/ricardotrevisan/ricardotrevisan.github.io/master/images/image_1_20251216.png
tags: hashnode, finance, riskmanagement, aiinfinance, marketstructure, quanttrading, decisiongovernance

---

## Why modeling markets as forecasting problems is a classic mistake

For a long time, the dominant approach to handling price series in the financial market was relatively consensual: collect historical data, extract features — prices, volumes, technical indicators — and apply statistical models or learning algorithms to try to forecast the market's future behavior.

This path recurs in classical econometrics, in time-series forecasting, and, more recently, in modern architectures like ARIMA, VAR, GARCH, LSTM and Transformers. Most of the academic and applied literature focuses on predicting return, direction, or volatility, treating price as a stochastic process to be estimated.

The implicit framing has always been the same:

> price as a stochastic process to be forecast

Given this framing, the central question arises almost automatically:

> **How much will the price go up or down?**

This is, in fact, the starting point of the first phase of almost every quant. Competent engineers, physicists, data scientists, and very smart people follow this same mental path. The intuition is simple: price is a time series; time series are forecasted; therefore, the market should be predictable. You are not the exception — you are the rule.

This intuition may sound elegant, logical, and technically sophisticated. And precisely for that reason it is so dangerous. It is seductive — and structurally wrong.

---

## Why it seems that “everyone forecasts”

The sense that forecasting is the correct path doesn’t happen by chance. It’s reinforced by the very technical ecosystem. Forecasting is teachable, publishable, backtestable, and easily translated into clear numbers, elegant charts, and synthetic metrics like Sharpe, CAGR, and drawdown. Forecasting yields a clean narrative: the model forecasts, the market responds.

In contrast, contextual reading, veto, NO\_TRADE, governance, and explicit risk management are difficult to formalize and even harder to sell. They don’t fit neatly in papers or dashboards. The technical environment rewards those who forecast, not those who govern.

There’s also a less explicit but decisive psychological factor: the belief that “those who really succeed don’t reveal the formula.” A myth of a secret model, a hidden architecture, a mathematical trick that works but doesn’t appear anywhere is created. When the system fails, the conclusion isn’t that the question is wrong, but that true knowledge hasn’t yet been reached.

The effect is pernicious. The less the system works, the more sophisticated it must look. More features, more models, more layers, more tuning. The premise error isn’t abandoned — it’s internalized as an individual failure.

Mature professional desks rarely describe their systems as direct price forecasting. Not because there’s a hidden oracle, but because what actually works doesn’t translate well into magical formulas. Governance and decision-making under uncertainty don’t fit in a sleek notebook. The absence of a “revealed formula” is interpreted, incorrectly, as evidence that it exists.

---

## The traditional role of Analytics (and its limit)

In this paradigm, Analytics serves to adjust parameters, optimize aggregated metrics, validate hypotheses via backtests, and refine forecasting models. It acts in the service of expectation: improving continuous estimates of expected return.

When the object being estimated isn’t stable, analytics becomes post-hoc sophistication. It explains the past but doesn’t protect the future. Metrics improve, confidence grows, while structural risk remains invisible. Analytics doesn’t govern the system — it merely decorates it.

Machine Learning amplifies this effect. More complex models reinforce the belief that, if something fails, the problem can only lie in insufficient data or inadequate architecture, never in the framing of the question. With the advance of generative AIs, this illusion intensifies: if models solve language and vision, they should solve markets — just scale.

The result is a dangerous shift: more computational intelligence, less epistemic honesty.

---

## The premise error

The problem isn’t with the algorithms, nor with the data, nor with a lack of sophistication. It’s in the **question**.

Regression, optimization, and RL models start from the implicit assumption that there exists a stable function mapping state to expected return. In the financial market, that function doesn’t exist stably because the environment is:

* **non-stationary**
    
* **adversarial**
    
* **reflexive**
    

The outcome is known: models work in backtests, survive some time in production, and collapse. Not by chance, but because they tried to estimate something that isn’t a stable object in the world.

The cruelest aspect is that this failure is felt before it’s understood. Something “doesn’t add up,” performance degrades, behavior changes — until it’s clear, too late, that the problem was never fine-tuning, but the premise.

---

## From “how much” to “whether”: the real shift

When a system is non-stationary, adversarial, and reflexive, the problem isn’t to forecast better. The problem becomes deciding whether an action should exist in that context.

This shifts the problem from continuous regression to state classification with veto. You don’t estimate outcomes; you govern possibility.

> You stopped estimating outcomes and started classifying states of the world.

This turn isn’t abstract. It materializes in architecture.

---

## What’s the answer then? A multi-agent architecture oriented to classification

The system isn’t an “intelligent agent” trying to forecast the market. It’s a **multi-agent system that classifies signals, setups, and context**, in hierarchical decision layers.

* A first agent operates exclusively on reading raw signals: price, flow, curvature, volatility, time, microstructure. It doesn’t decide. It describes the state.
    
* A second agent works on that description and **classifies the setup**: valid reversion, structural continuation, weak breakout, exhaustion, noise. Here there’s no return expectation, only coherence among signals.
    
* A third agent acts as a **validation and veto layer**, evaluating risk, asymmetry, market context, signal conflicts, and cost of error. This agent doesn’t optimize PnL; it decides whether the action is allowed, reduced, or blocked.
    

The final decision doesn’t emerge from a single score, but from the **intersection of independent classifications**. When there’s conflict, the system doesn’t force consensus — it vetoes.

NO\_TRADE isn’t a failure. It’s a valid output.

This design eliminates the illusion of a continuous optimum point and replaces it with explicit governance. Each agent has a limited, auditable, and replaceable function. The system doesn’t “learn to forecast”; it learns to not place itself in bad states.

---

## Classification ≠ heuristic

This isn’t heuristic in disguise. The classes aren’t arbitrary labels. They’re operational constructs with explicit consequences. Each classification implies permissions, restrictions, and clear risk policies.

Heuristic decides without explaining. Operational classification decides and enables auditing.

---

## The role of AI in this system

AI doesn’t disappear. It shifts function.

Statistical models and ML assist in estimating intermediate variables — regime, volatility, persistence — but never decide directly.

LLMs act as a cognitive layer: consolidating heterogeneous signals, explaining decisions, aiding context reading, and functioning as semantic validation. They’re not oracles. They’re instruments subordinated to governance.

---

## Why this works better

Classification tolerates non-stationarity. A veto is robust to adversarial conditions. State reading respects reflexivity.

Errors appear early. Learning stops being illusory and becomes institutional. Alpha stops being aggressive and becomes defensive. Fewer trades, less self-deception, more survivability.

This kind of alpha doesn’t impress in backtests. But it’s the only one that ages well.

---

## Conclusion

Maturity in decision systems doesn’t come from forecasting better. It comes from knowing when not to act.

> Markets don’t reward those who estimate the future best. They reward those who better avoid the wrong states of the present.

This is the premise shift. And it’s exactly what our model implements.

#Finance #QuantTrading #RiskManagement #DecisionGovernance