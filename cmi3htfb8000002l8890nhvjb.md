---
title: "AI #CognitiveComputing #MachineLearning"
datePublished: Mon Nov 17 2025 18:43:37 GMT+0000 (Coordinated Universal Time)
cuid: cmi3htfb8000002l8890nhvjb
slug: ai-cognitivecomputing-machinelearning
cover: https://raw.githubusercontent.com/ricardotrevisan/ricardotrevisan.github.io/master/images/image_20251104.png
tags: ai, machinelearning, cognitivecomputing

---

Remember that topic?

When two people say that, they don‚Äôt relive every frame of the past.
They revisit relevant traces of their shared history ‚Äî evoke contexts, choose what matters to evolve the current conversation.

Now imagine two LLMs doing the same.

In the world of language models, ‚Äúremembering‚Äù is about cached representations (K/V) ‚Äî the semantic memory space where the model stores representations of what it has processed.
A Query (Q) is the trigger: ‚Äúhey, remember this ‚Äî it might matter now.‚Äù

That shared memory fuels attention, reduces latency and cost, and opens space for something new: models talking directly to each other, without intermediaries.

Research from Tsinghua University shows that, to evolve logical and cumulative reasoning, it‚Äôs not enough to have a Chain of Thought (CoT) or Tree of Thought (ToT).
What you need is a memory space ‚Äî an organized space to store intermediate results.

It would be like dialoguing without preserving prior answers in an organized and known space ‚Äî we‚Äôd quickly lose the thread of the narrative.
The Cumulative Reasoning (CR) framework from Tsinghua introduces this ‚Äúmemory space‚Äù, dramatically improving inference capabilities.

Essentially, each LLM consults what the other has already ‚Äúlived‚Äù, sharing context without reprocessing everything. Less redundancy, more fluency, more collective intelligence.

The challenge ‚Äî and the beauty ‚Äî is in deciding how much history to carry and at what level of detail. We do this naturally in our conversations: surface only what adds sense to the next step.

It‚Äôs interesting how innovation is closely tied to how we understand ourselves.

The success of these frameworks depends on that subtle balance between what needs to be remembered ‚Äî and what deserves to be forgotten. Like life itself, right?

üìö **Sources:**
‚Ä¢ Marktechpost - C2C: https://www.marktechpost.com/2025/11/04/cache-to-cachec2c-direct-semantic-communication-between-large-language-models-via-kv-cache-fusion/

Hashtags
#AI #CognitiveComputing #MachineLearning